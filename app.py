# -*- coding: utf-8 -*-
"""Loan Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KJJtrrYQLrSu8wyO_jht7vFkMp7d6H1H

##Importing the Required Libraries
"""

# importing required libraries
import numpy as np
import pandas as pd
import streamlit as st

# to visualizing
import matplotlib.pyplot as plt
import seaborn as sns

# importing libraries from Scikit learn for preprocessiong and encoding
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler

# to build ML models
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
import xgboost as xgb

# for model evaluation
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score

# to handle missing values
from sklearn.impute import SimpleImputer

#for cross validation
from sklearn.model_selection import cross_val_score, GridSearchCV

# to save the model
import pickle
# to ignore warnings
import warnings
warnings.filterwarnings('ignore')

data = pd.read_csv('https://raw.githubusercontent.com/chandra1024448/loan-prediction-app/main/Loan%20Prediction.csv', encoding='ISO-8859-1')

st.title("Loan Prediction App üöÄ")
st.subheader("Dataset Preview")
st.write("data.head()")



"""## Data Preprocessing"""

st.write(data.head())

st.write("Shape of the dataset:", data.shape)

st.write("Descriptive Statistics:")
st.write(data.describe())

st.write("Missing values in each column:")
st.write(data.isnull().sum())

data.info()

data1 = (data['CITY'].value_counts().sum() - data['CITY'].duplicated().sum())
print(data1)

# Basic preprocessing
data.dropna(inplace=True)

# Encode categorical columns
le = LabelEncoder()
for col in data.select_dtypes(include='object').columns:
    data[col] = le.fit_transform(data[col])

# Split data
st.write("Columns in dataset:", data.columns.tolist())
X = data.drop('Risk_Flag', axis=1)
y = data['Risk_Flag']
model = LogisticRegression(max_iter=1000)
model.fit(X, y)

# User Input UI
st.sidebar.header("Enter Applicant Details")

gender = st.sidebar.selectbox("Gender", ['Male', 'Female'])
married = st.sidebar.selectbox("Married", ['Yes', 'No'])
education = st.sidebar.selectbox("Education", ['Graduate', 'Not Graduate'])
self_employed = st.sidebar.selectbox("Self Employed", ['Yes', 'No'])
applicant_income = st.sidebar.number_input("Applicant Income", min_value=0)
coapplicant_income = st.sidebar.number_input("Coapplicant Income", min_value=0)
loan_amount = st.sidebar.slider("Loan Amount", min_value=0, max_value=700)
loan_term = st.sidebar.slider("Loan Term (in days)", min_value=0, max_value=480)
credit_history = st.sidebar.selectbox("Credit History", [1.0, 0.0])
property_area = st.sidebar.selectbox("Property Area", ['Urban', 'Semiurban', 'Rural'])

# Encode user inputs
input_dict = {
    'Gender': 1 if gender == 'Male' else 0,
    'Married': 1 if married == 'Yes' else 0,
    'Education': 1 if education == 'Graduate' else 0,
    'Self_Employed': 1 if self_employed == 'Yes' else 0,
    'ApplicantIncome': applicant_income,
    'CoapplicantIncome': coapplicant_income,
    'LoanAmount': loan_amount,
    'Loan_Amount_Term': loan_term,
    'Credit_History': credit_history,
    'Property_Area': {'Urban': 2, 'Semiurban': 1, 'Rural': 0}[property_area]
}

input_df = pd.DataFrame([input_dict])

# Predict
prediction = model.predict(input_df)[0]
result = "Approved ‚úÖ" if prediction == 1 else "Rejected ‚ùå"

# Show result
st.subheader("Prediction Result")
st.write(f"Your loan is likely to be: *{result}*")
"""##EDA

###Univariate Analysis
"""

# defining the figure size
plt.figure(figsize=(15, 12)) # Adjusted figure size

n_features = len(data.columns)
n_cols = 4 # Number of columns you want in the subplot grid
n_rows = (n_features + n_cols - 1) // n_cols # Calculate the number of rows needed

for i, feature in enumerate(data.columns):
  plt.subplot(n_rows, n_cols, i+1)
  sns.histplot(data=data, x =feature)
  plt.xlabel(feature)
  plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

"""###Bivariate Analysis"""

if 'ID' in data.columns:
    data = data.drop('ID', axis=1)

# Convert categorical variables
data = pd.get_dummies(data)

# Replace NaNs
data = data.fillna(0)

TARGET_COLUMN = 'Risk_Flag'  # or whichever is correct
X = data.drop(TARGET_COLUMN, axis=1)
y = data[TARGET_COLUMN]

# plotting the countplot for all features
plt.figure(figsize=(15, 12))
n_features = len(data.columns)
n_cols = 4
n_rows = (n_features + n_cols - 1) // n_cols

for i, feature in enumerate(data.columns):
  plt.subplot(n_rows, n_cols, i+1)
  sns.countplot(data=data, x=feature, hue='Risk_Flag')
  plt.xlabel(feature)
  plt.ylabel('Count')
  plt.legend(['Not Defaulted', 'Defaulted'])

plt.tight_layout()
plt.show()

# plotting the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(data.corr(), annot=True, cmap='coolwarm')
plt.show()

"""##Model Building"""

# splitting the data into train and test sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

"""###1. Model1 - Logistic Regression"""

from imblearn.over_sampling import SMOTE

# scale your features
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

# applying SMOTE to training data
smote = SMOTE(random_state=42)
x_train_balanced ,y_train_balanced =smote.fit_resample(x_train, y_train)

# checking new class destribution
print("After SMOTE:", np.bincount(y_train_balanced))

model1 = LogisticRegression(
    max_iter=1000,
    random_state=42)

model1.fit(x_train_balanced, y_train_balanced)

# to predict and evaluate
y_pred = model1.predict(x_test)

# accuracy score
print('Accuracy:', accuracy_score(y_test, y_pred))

# classification report
print('Classification Report:', classification_report(y_test, y_pred))

print('Confusion Matrix:', confusion_matrix(y_test, y_pred))

# plotting the confusion matrix
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show();

st.subheader("Predict Loan Risk")
user_input = []
for col in X.columns:
    val = st.number_input(f"Enter value for {col}", value=0.0)
    user_input.append(val)

if st.button("Predict"):
    prediction = model.predict([user_input])
    st.write("Prediction:", int(prediction[0]))

"""###2. Model2 - Random Forest"""

# train the model
model2 = RandomForestClassifier(random_state=42)
model2.fit(x_train_balanced, y_train_balanced)

# predicting on test data
y_model2_pred = model2.predict(x_test)

# evaluation
print('Accuracy:', accuracy_score(y_test, y_model2_pred))
print('Classification Report:', classification_report(y_test, y_model2_pred))
print('Confusion Matrix:', confusion_matrix(y_test, y_model2_pred))

# visualising confusion matrix
sns.heatmap(confusion_matrix(y_test, y_model2_pred), annot=True, cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Random Forest Confusion Matrix')
plt.show();

# tune hyperparameters
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5],
    'class_weight': ['balanced']
}

grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=3, scoring='f1')
grid_search.fit(x_train_balanced, y_train_balanced)

best_model = grid_search.best_estimator_

"""###3. Model3 - XGBoost"""

# training the xgboost model
xgb.model3 = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')
xgb.model3.fit(x_train_balanced, y_train_balanced)

# predict nd evaluate
y_model3_pred = xgb.model3.predict(x_test)

print('Accuracy:', accuracy_score(y_test, y_model3_pred))
print('Classification Report:', classification_report(y_test, y_model3_pred))
print('Confusion Matrix:', confusion_matrix(y_test, y_model3_pred))

# plotting the confusion matrics
sns.heatmap(confusion_matrix(y_test, y_model3_pred), annot=True, cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('XGBoost Confusion Matrix')
plt.show();

# Predictions
log_pred = model1.predict(x_test)
rf_pred = model2.predict(x_test)
xgb_pred = xgb.model3.predict(x_test)

# Store all metrics
from sklearn.metrics import precision_score, recall_score

results = {
    'Model': ['Logistic Regression', 'Random Forest', 'XGBoost'],
    'Accuracy': [
        accuracy_score(y_test, log_pred),
        accuracy_score(y_test, rf_pred),
        accuracy_score(y_test, xgb_pred)
    ],
    'Precision': [
        precision_score(y_test, log_pred),
        precision_score(y_test, rf_pred),
        precision_score(y_test, xgb_pred)
    ],
    'Recall': [
        recall_score(y_test, log_pred),
        recall_score(y_test, rf_pred),
        recall_score(y_test, xgb_pred)
    ],
    'F1 Score': [
        f1_score(y_test, log_pred),
        f1_score(y_test, rf_pred),
        f1_score(y_test, xgb_pred)
    ]
}

# Display as DataFrame
comparison_df = pd.DataFrame(results)
display(comparison_df)

comparison_df.set_index('Model').plot(kind='bar', figsize=(10, 6))
plt.title("Model Comparison")
plt.ylabel("Score")
plt.ylim(0, 1)
plt.legend(loc='lower right')
plt.xticks(rotation=0)
plt.grid(True)
plt.show()

"""##Saving the best model - XGBoost"""

import joblib
joblib.dump(xgb.model3, 'xgb_model.pkl')

from xgboost import plot_importance

plt.figure(figsize=(12, 6))
plot_importance(xgb.model3, max_num_features=10)  # Top 10 features
plt.title('Top 10 Important Features - XGBoost')
plt.show()

"""##Creating a clean data pipeline"""

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

final_pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('model', xgb.model3)
])

joblib.dump(final_pipeline, 'xgboost_pipeline.pkl')

import streamlit as st
import pandas as pd
import joblib
import numpy as np
import matplotlib.pyplot as plt
from xgboost import plot_importance

# Load the trained XGBoost model
model = joblib.load('xgboost_pipeline.pkl')  # Or 'xgboost_pipeline.pkl' if using pipeline

# Title
st.title("Loan Default Risk Predictor üè¶")

st.markdown("Enter applicant's loan details below to check if they're at *High Risk* or *Low Risk*.")

# Input form
with st.form("loan_form"):
    income = st.number_input("Annual Income (USD)", min_value=0)
    age = st.slider("Age", 18, 75, 30)
    experience = st.slider("Years of Work Experience", 0, 50, 5)
    married = st.selectbox("Marital Status", ["Yes", "No"])
    house_ownership = st.selectbox("House Ownership", ["Rented", "Owned", "Neither"])
    car_owned = st.selectbox("Owns a Car?", ["Yes", "No"])
    current_job_years = st.slider("Years in Current Job", 0, 20, 2)
    current_house_years = st.slider("Years in Current House", 0, 30, 5)
    city_code = st.number_input("City Code (integer)", min_value=0)
    state_code = st.number_input("State Code (integer)", min_value=0)

    submitted = st.form_submit_button("Predict")

if submitted:
    # Map categorical to numerical (ensure this matches your training data!)
    married = 1 if married == "Yes" else 0
    car_owned = 1 if car_owned == "Yes" else 0
    house_dict = {"Rented": 0, "Owned": 1, "Neither": 2}
    house_ownership = house_dict[house_ownership]

    # Create dataframe
    input_data = pd.DataFrame([[
        income, age, experience, married, house_ownership,
        car_owned, current_job_years, current_house_years, city_code, state_code
    ]], columns=[
        "income", "age", "experience", "married", "house_ownership", "car_owned",
        "current_job_years", "current_house_years", "city", "state"
    ])

    # Predict
    prediction = model.predict(input_data)[0]
    prob = model.predict_proba(input_data)[0][1]  # Probability of being high risk

    # Result
    st.subheader("üîç Prediction Result")
    if prediction == 1:
        st.error(f"‚ö†Ô∏è High Risk of Loan Default with probability {prob:.2f}")
    else:
        st.success(f"‚úÖ Low Risk of Loan Default with probability {1 - prob:.2f}")

    # Feature importance plot
    st.subheader("üìä Feature Importance")
    fig, ax = plt.subplots(figsize=(10, 6))
    plot_importance(model.named_steps['model'], max_num_features=10, height=0.5, ax=ax)
    st.pyplot(fig)

