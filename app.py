# -*- coding: utf-8 -*-
"""Loan Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KJJtrrYQLrSu8wyO_jht7vFkMp7d6H1H

##Importing the Required Libraries
"""

# importing required libraries
import numpy as np
import pandas as pd
import streamlit as st

# to visualizing
import matplotlib.pyplot as plt
import seaborn as sns

# importing libraries from Scikit learn for preprocessiong and encoding
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler

# to build ML models
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
import xgboost as xgb

# for model evaluation
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score

# to handle missing values
from sklearn.impute import SimpleImputer

#for cross validation
from sklearn.model_selection import cross_val_score, GridSearchCV

# to save the model
import pickle
# to ignore warnings
import warnings
warnings.filterwarnings('ignore')

data = pd.read_csv('https://raw.githubusercontent.com/chandra1024448/loan-prediction-app/main/Loan%20Prediction.csv', encoding='ISO-8859-1')

st.title("Loan Prediction App üöÄ")

"""## Data Preprocessing"""

st.write(data.head())

st.write("Shape of the dataset:", data.shape)

st.write("Descriptive Statistics:")
st.write(data.describe())

st.write("Missing values in each column:")
st.write(data.isnull().sum())

data.info()

# Basic preprocessing
data.dropna(inplace=True)

# Encode categorical columns
le = LabelEncoder()
for col in data.select_dtypes(include='object').columns:
    data[col] = le.fit_transform(data[col])

# Split data
st.write("Columns in dataset:", data.columns.tolist())
X = data.drop('Risk_Flag', axis=1)
y = data['Risk_Flag']
model = LogisticRegression(max_iter=1000)
model.fit(X, y)

# User Input UI
st.sidebar.header("Applicant Details")

import pickle

# Example encoders for your categorical columns
from sklearn.preprocessing import LabelEncoder

married_encoder = LabelEncoder()
data["Married/Single"] = married_encoder.fit_transform(data["Married/Single"])

house_encoder = LabelEncoder()
data["House_Ownership"] = house_encoder.fit_transform(data["House_Ownership"])

car_encoder = LabelEncoder()
data["Car_Ownership"] = car_encoder.fit_transform(data["Car_Ownership"])

profession_encoder = LabelEncoder()
data["Profession"] = profession_encoder.fit_transform(data["Profession"])

city_encoder = LabelEncoder()
data["CITY"] = city_encoder.fit_transform(data["CITY"])

state_encoder = LabelEncoder()
data["STATE"] = state_encoder.fit_transform(data["STATE"])

# Save all encoders
with open("encoders.pkl", "wb") as f:
    pickle.dump({
        "married": married_encoder,
        "house": house_encoder,
        "car": car_encoder,
        "profession": profession_encoder,
        "city": city_encoder,
        "state": state_encoder
    }, f)

with open("logistic_model.pkl", "wb") as f:
    pickle.dump(model, f)

import pickle

# Load the model
model_url = "https://raw.githubusercontent.com/chandra1024448/loan-prediction-app/main/logistic_model.pkl"
model = pickle.load(requests.get(model_url, stream=True).raw)

# Load the encoders
encoder_url = "https://raw.githubusercontent.com/chandra1024448/loan-prediction-app/main/encoders.pkl"
encoders = pickle.load(requests.get(encoder_url, stream=True).raw)

# Encode categorical values using the same encoders
input_df["Married/Single"] = encoders["married"].transform(input_df["Married/Single"])
input_df["House_Ownership"] = encoders["house"].transform(input_df["House_Ownership"])
input_df["Car_Ownership"] = encoders["car"].transform(input_df["Car_Ownership"])
input_df["Profession"] = encoders["profession"].transform(input_df["Profession"])
input_df["CITY"] = encoders["city"].transform(input_df["CITY"])
input_df["STATE"] = encoders["state"].transform(input_df["STATE"])

# Predict
prediction = model.predict(input_df)[0]

# Display result
if prediction == 1:
    st.success("üö® High Risk: Loan might default")
else:
    st.success("‚úÖ Low Risk: Loan is likely safe")


st.subheader("Exploratory Data Analysis (EDA)")

# Dataset overview
st.write("### Dataset Overview")
st.dataframe(data.head())

# Shape of dataset
st.write("*Shape of dataset:*", data.shape)

# Summary statistics
st.write("### Summary Statistics")
st.dataframe(data.describe())

# Data types and null values
st.write("### Data Info")
buffer = io.StringIO()
data.info(buf=buffer)
s = buffer.getvalue()
st.text(s)

# Missing values
st.write("### Missing Values")
st.dataframe(data.isnull().sum())

# Plotting distributions
st.write("### Distribution of Target Variable")
st.bar_chart(data['Risk_Flag'].value_counts())

# More visualizations (optional)
# st.write("### Income Distribution")
# st.hist(data['Income'])  # Example, if column exists

"""##EDA

import matplotlib.pyplot as plt
import seaborn as sns

# EDA Visuals Section
st.subheader("üìä Exploratory Data Analysis")

if st.checkbox("Show EDA Visuals"):
    # Plot 1: Risk_Flag distribution
    st.write("### 1. Risk_Flag Distribution")
    fig1, ax1 = plt.subplots()
    sns.histplot(data['Risk_Flag], ax=ax1)
    st.pyplot(fig1)

    # Plot 2: Income distribution
    st.write("### 2. Income Distribution")
    fig2, ax2 = plt.subplots()
    sns.histplot(data['Income'], kde=True, ax=ax2)
    st.pyplot(fig2)

    # Plot 3: Age vs Income
    st.write("### 3. Age vs Income")
    fig3, ax3 = plt.subplots()
    sns.scatterplot(x='Age', y='Income', data=data, ax=ax3)
    st.pyplot(fig3)

    # Plot 4: Countplot of House Ownership
    st.write("### 4. House Ownership Count")
    fig4, ax4 = plt.subplots()
    sns.countplot(data=data, x='House_Ownership', ax=ax4)
    st.pyplot(fig4)

    # Plot 5: Car Ownership by Risk Flag
    st.write("### 5. Car Ownership vs Risk")
    fig5, ax5 = plt.subplots()
    sns.countplot(data=data, x='Car_Ownership', hue='Risk_Flag', ax=ax5)
    st.pyplot(fig5)

    # Plot 6: Correlation heatmap
    st.write("### 6. Correlation Heatmap")
    fig6, ax6 = plt.subplots(figsize=(10,6))
    sns.heatmap(data.corr(numeric_only=True), annot=True, cmap='coolwarm', ax=ax6)
    st.pyplot(fig6)

###Univariate Analysis
"""

# defining the figure size
plt.figure(figsize=(15, 12)) # Adjusted figure size

n_features = len(data.columns)
n_cols = 4 # Number of columns you want in the subplot grid
n_rows = (n_features + n_cols - 1) // n_cols # Calculate the number of rows needed

for i, feature in enumerate(data.columns):
  plt.subplot(n_rows, n_cols, i+1)
  sns.histplot(data=data, x =feature)
  plt.xlabel(feature)
  plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

"""###Bivariate Analysis"""

if 'ID' in data.columns:
    data = data.drop('ID', axis=1)

# Convert categorical variables
data = pd.get_dummies(data)

# Replace NaNs
data = data.fillna(0)

TARGET_COLUMN = 'Risk_Flag'  # or whichever is correct
X = data.drop(TARGET_COLUMN, axis=1)
y = data[TARGET_COLUMN]

# plotting the countplot for all features
plt.figure(figsize=(15, 12))
n_features = len(data.columns)
n_cols = 4
n_rows = (n_features + n_cols - 1) // n_cols

for i, feature in enumerate(data.columns):
  plt.subplot(n_rows, n_cols, i+1)
  sns.countplot(data=data, x=feature, hue='Risk_Flag')
  plt.xlabel(feature)
  plt.ylabel('Count')
  plt.legend(['Not Defaulted', 'Defaulted'])

plt.tight_layout()
plt.show()

# plotting the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(data.corr(), annot=True, cmap='coolwarm')
plt.show()

"""##Model Building"""

# splitting the data into train and test sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

"""###1. Model1 - Logistic Regression"""

from imblearn.over_sampling import SMOTE

# scale your features
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

# applying SMOTE to training data
smote = SMOTE(random_state=42)
x_train_balanced ,y_train_balanced =smote.fit_resample(x_train, y_train)

# checking new class destribution
print("After SMOTE:", np.bincount(y_train_balanced))

model1 = LogisticRegression(
    max_iter=1000,
    random_state=42)

model1.fit(x_train_balanced, y_train_balanced)

# to predict and evaluate
y_pred = model1.predict(x_test)

# accuracy score
print('Accuracy:', accuracy_score(y_test, y_pred))

# classification report
print('Classification Report:', classification_report(y_test, y_pred))

print('Confusion Matrix:', confusion_matrix(y_test, y_pred))

# plotting the confusion matrix
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show();

st.subheader("Predict Loan Risk")
user_input = []
for col in X.columns:
    val = st.number_input(f"Enter value for {col}", value=0.0)
    user_input.append(val)

if st.button("Predict"):
    prediction = model.predict([user_input])
    st.write("Prediction:", int(prediction[0]))

"""###2. Model2 - Random Forest"""

# train the model
model2 = RandomForestClassifier(random_state=42)
model2.fit(x_train_balanced, y_train_balanced)

# predicting on test data
y_model2_pred = model2.predict(x_test)

# evaluation
print('Accuracy:', accuracy_score(y_test, y_model2_pred))
print('Classification Report:', classification_report(y_test, y_model2_pred))
print('Confusion Matrix:', confusion_matrix(y_test, y_model2_pred))

# visualising confusion matrix
sns.heatmap(confusion_matrix(y_test, y_model2_pred), annot=True, cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Random Forest Confusion Matrix')
plt.show();

# tune hyperparameters
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5],
    'class_weight': ['balanced']
}

grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=3, scoring='f1')
grid_search.fit(x_train_balanced, y_train_balanced)

best_model = grid_search.best_estimator_

"""###3. Model3 - XGBoost"""

# training the xgboost model
xgb.model3 = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')
xgb.model3.fit(x_train_balanced, y_train_balanced)

# predict nd evaluate
y_model3_pred = xgb.model3.predict(x_test)

print('Accuracy:', accuracy_score(y_test, y_model3_pred))
print('Classification Report:', classification_report(y_test, y_model3_pred))
print('Confusion Matrix:', confusion_matrix(y_test, y_model3_pred))

# plotting the confusion matrics
sns.heatmap(confusion_matrix(y_test, y_model3_pred), annot=True, cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('XGBoost Confusion Matrix')
plt.show();

# Predictions
log_pred = model1.predict(x_test)
rf_pred = model2.predict(x_test)
xgb_pred = xgb.model3.predict(x_test)

# Store all metrics
from sklearn.metrics import precision_score, recall_score

results = {
    'Model': ['Logistic Regression', 'Random Forest', 'XGBoost'],
    'Accuracy': [
        accuracy_score(y_test, log_pred),
        accuracy_score(y_test, rf_pred),
        accuracy_score(y_test, xgb_pred)
    ],
    'Precision': [
        precision_score(y_test, log_pred),
        precision_score(y_test, rf_pred),
        precision_score(y_test, xgb_pred)
    ],
    'Recall': [
        recall_score(y_test, log_pred),
        recall_score(y_test, rf_pred),
        recall_score(y_test, xgb_pred)
    ],
    'F1 Score': [
        f1_score(y_test, log_pred),
        f1_score(y_test, rf_pred),
        f1_score(y_test, xgb_pred)
    ]
}

# Display as DataFrame
comparison_df = pd.DataFrame(results)
display(comparison_df)

comparison_df.set_index('Model').plot(kind='bar', figsize=(10, 6))
plt.title("Model Comparison")
plt.ylabel("Score")
plt.ylim(0, 1)
plt.legend(loc='lower right')
plt.xticks(rotation=0)
plt.grid(True)
plt.show()

"""##Saving the best model - XGBoost"""

import joblib
joblib.dump(xgb.model3, 'xgb_model.pkl')

from xgboost import plot_importance

plt.figure(figsize=(12, 6))
plot_importance(xgb.model3, max_num_features=10)  # Top 10 features
plt.title('Top 10 Important Features - XGBoost')
plt.show()

"""##Creating a clean data pipeline"""

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

final_pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('model', xgb.model3)
])

joblib.dump(final_pipeline, 'xgboost_pipeline.pkl')

import streamlit as st
import pandas as pd
import joblib
import numpy as np
import matplotlib.pyplot as plt
from xgboost import plot_importance

# Load the trained XGBoost model
model = joblib.load('xgboost_pipeline.pkl')  # Or 'xgboost_pipeline.pkl' if using pipeline

# Title
st.title("Loan Default Risk Predictor üè¶")

st.markdown("Enter applicant's loan details below to check if they're at *High Risk* or *Low Risk*.")

# Input form
with st.form("loan_form"):
    income = st.number_input("Annual Income (USD)", min_value=0)
    age = st.slider("Age", 18, 75, 30)
    experience = st.slider("Years of Work Experience", 0, 50, 5)
    married = st.selectbox("Marital Status", ["Yes", "No"])
    house_ownership = st.selectbox("House Ownership", ["Rented", "Owned", "Neither"])
    car_owned = st.selectbox("Owns a Car?", ["Yes", "No"])
    current_job_years = st.slider("Years in Current Job", 0, 20, 2)
    current_house_years = st.slider("Years in Current House", 0, 30, 5)
    city_code = st.number_input("City Code (integer)", min_value=0)
    state_code = st.number_input("State Code (integer)", min_value=0)

    submitted = st.form_submit_button("Predict")

if submitted:
    # Map categorical to numerical (ensure this matches your training data!)
    married = 1 if married == "Yes" else 0
    car_owned = 1 if car_owned == "Yes" else 0
    house_dict = {"Rented": 0, "Owned": 1, "Neither": 2}
    house_ownership = house_dict[house_ownership]

    # Create dataframe
    input_data = pd.DataFrame([[
        income, age, experience, married, house_ownership,
        car_owned, current_job_years, current_house_years, city_code, state_code
    ]], columns=[
        "income", "age", "experience", "married", "house_ownership", "car_owned",
        "current_job_years", "current_house_years", "city", "state"
    ])

    # Predict
    prediction = model.predict(input_data)[0]
    prob = model.predict_proba(input_data)[0][1]  # Probability of being high risk

    # Result
    st.subheader("üîç Prediction Result")
    if prediction == 1:
        st.error(f"‚ö†Ô∏è High Risk of Loan Default with probability {prob:.2f}")
    else:
        st.success(f"‚úÖ Low Risk of Loan Default with probability {1 - prob:.2f}")

    # Feature importance plot
    st.subheader("üìä Feature Importance")
    fig, ax = plt.subplots(figsize=(10, 6))
    plot_importance(model.named_steps['model'], max_num_features=10, height=0.5, ax=ax)
    st.pyplot(fig)

