# -*- coding: utf-8 -*-
"""Loan Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KJJtrrYQLrSu8wyO_jht7vFkMp7d6H1H

##Importing the Required Libraries
"""

# importing required libraries
import numpy as np
import pandas as pd

# to visualizing
import matplotlib.pyplot as plt
import seaborn as sns

# importing libraries from Scikit learn for preprocessiong and encoding
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler

# to build ML models
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
import xgboost as xgb

# for model evaluation
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score

# to handle missing values
from sklearn.impute import SimpleImputer

#for cross validation
from sklearn.model_selection import cross_val_score, GridSearchCV

# to save the model
import pickle
# to ignore warnings
import warnings
warnings.filterwarnings('ignore')

# accessing the file
from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Practice Projects/Files/Loan Prediction.csv')

"""## Data Preprocessing"""

data.head()

data.shape

data.isnull().sum()

data.describe()

data.info()

data1 = (data['CITY'].value_counts().sum() - data['CITY'].duplicated().sum())
print(data1)

print(data['Married/Single'].value_counts())
print(data['House_Ownership'].value_counts())
print(data['Car_Ownership'].value_counts())
print(data['Profession'].value_counts())
print(data['CITY'].value_counts())
print(data['STATE'].value_counts())

# converting categorical values into integer
le = LabelEncoder()
data['Married/Single'] = le.fit_transform(data['Married/Single'])
data['House_Ownership'] = le.fit_transform(data['House_Ownership'])
data['Car_Ownership'] = le.fit_transform(data['Car_Ownership'])
data['Profession'] = le.fit_transform(data['Profession'])
data['CITY'] = le.fit_transform(data['CITY'])
data['STATE'] = le.fit_transform(data['STATE'])

data.info()

"""##EDA

###Univariate Analysis
"""

# defining the figure size
plt.figure(figsize=(15, 12)) # Adjusted figure size

n_features = len(data.columns)
n_cols = 4 # Number of columns you want in the subplot grid
n_rows = (n_features + n_cols - 1) // n_cols # Calculate the number of rows needed

for i, feature in enumerate(data.columns):
  plt.subplot(n_rows, n_cols, i+1)
  sns.histplot(data=data, x =feature)
  plt.xlabel(feature)
  plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

"""###Bivariate Analysis"""

x = data.drop(columns=['Risk_Flag'], axis = 1)
y = data['Risk_Flag']

data.shape

# plotting the countplot for all features
plt.figure(figsize=(15, 12))
n_features = len(data.columns)
n_cols = 4
n_rows = (n_features + n_cols - 1) // n_cols

for i, feature in enumerate(data.columns):
  plt.subplot(n_rows, n_cols, i+1)
  sns.countplot(data=data, x=feature, hue='Risk_Flag')
  plt.xlabel(feature)
  plt.ylabel('Count')
  plt.legend(['Not Defaulted', 'Defaulted'])

plt.tight_layout()
plt.show()

# plotting the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(data.corr(), annot=True, cmap='coolwarm')
plt.show()

"""##Model Building"""

# splitting the data into train and test sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

"""###1. Model1 - Logistic Regression"""

from imblearn.over_sampling import SMOTE

# scale your features
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

# applying SMOTE to training data
smote = SMOTE(random_state=42)
x_train_balanced ,y_train_balanced =smote.fit_resample(x_train, y_train)

# checking new class destribution
print("After SMOTE:", np.bincount(y_train_balanced))

model1 = LogisticRegression(
    max_iter=1000,
    random_state=42)

model1.fit(x_train_balanced, y_train_balanced)

# to predict and evaluate
y_pred = model1.predict(x_test)

# accuracy score
print('Accuracy:', accuracy_score(y_test, y_pred))

# classification report
print('Classification Report:', classification_report(y_test, y_pred))

print('Confusion Matrix:', confusion_matrix(y_test, y_pred))

# plotting the confusion matrix
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show();

"""###2. Model2 - Random Forest"""

# train the model
model2 = RandomForestClassifier(random_state=42)
model2.fit(x_train_balanced, y_train_balanced)

# predicting on test data
y_model2_pred = model2.predict(x_test)

# evaluation
print('Accuracy:', accuracy_score(y_test, y_model2_pred))
print('Classification Report:', classification_report(y_test, y_model2_pred))
print('Confusion Matrix:', confusion_matrix(y_test, y_model2_pred))

# visualising confusion matrix
sns.heatmap(confusion_matrix(y_test, y_model2_pred), annot=True, cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Random Forest Confusion Matrix')
plt.show();

# tune hyperparameters
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5],
    'class_weight': ['balanced']
}

grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=3, scoring='f1')
grid_search.fit(x_train_balanced, y_train_balanced)

best_model = grid_search.best_estimator_

"""###3. Model3 - XGBoost"""

!pip install xgboost

# training the xgboost model
xgb.model3 = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')
xgb.model3.fit(x_train_balanced, y_train_balanced)

# predict nd evaluate
y_model3_pred = xgb.model3.predict(x_test)

print('Accuracy:', accuracy_score(y_test, y_model3_pred))
print('Classification Report:', classification_report(y_test, y_model3_pred))
print('Confusion Matrix:', confusion_matrix(y_test, y_model3_pred))

# plotting the confusion matrics
sns.heatmap(confusion_matrix(y_test, y_model3_pred), annot=True, cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('XGBoost Confusion Matrix')
plt.show();

# Predictions
log_pred = model1.predict(x_test)
rf_pred = model2.predict(x_test)
xgb_pred = xgb.model3.predict(x_test)

# Store all metrics
from sklearn.metrics import precision_score, recall_score

results = {
    'Model': ['Logistic Regression', 'Random Forest', 'XGBoost'],
    'Accuracy': [
        accuracy_score(y_test, log_pred),
        accuracy_score(y_test, rf_pred),
        accuracy_score(y_test, xgb_pred)
    ],
    'Precision': [
        precision_score(y_test, log_pred),
        precision_score(y_test, rf_pred),
        precision_score(y_test, xgb_pred)
    ],
    'Recall': [
        recall_score(y_test, log_pred),
        recall_score(y_test, rf_pred),
        recall_score(y_test, xgb_pred)
    ],
    'F1 Score': [
        f1_score(y_test, log_pred),
        f1_score(y_test, rf_pred),
        f1_score(y_test, xgb_pred)
    ]
}

# Display as DataFrame
comparison_df = pd.DataFrame(results)
display(comparison_df)

comparison_df.set_index('Model').plot(kind='bar', figsize=(10, 6))
plt.title("Model Comparison")
plt.ylabel("Score")
plt.ylim(0, 1)
plt.legend(loc='lower right')
plt.xticks(rotation=0)
plt.grid(True)
plt.show()

"""##Saving the best model - XGBoost"""

import joblib
joblib.dump(xgb.model3, 'xgb_model.pkl')

from xgboost import plot_importance

plt.figure(figsize=(12, 6))
plot_importance(xgb.model3, max_num_features=10)  # Top 10 features
plt.title('Top 10 Important Features - XGBoost')
plt.show()

"""##Creating a clean data pipeline"""

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

final_pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('model', xgb.model3)
])

joblib.dump(final_pipeline, 'xgboost_pipeline.pkl')

import streamlit as st
import pandas as pd
import joblib
import numpy as np
import matplotlib.pyplot as plt
from xgboost import plot_importance

# Load the trained XGBoost model
model = joblib.load('xgboost_pipeline.pkl')  # Or 'xgboost_pipeline.pkl' if using pipeline

# Title
st.title("Loan Default Risk Predictor üè¶")

st.markdown("Enter applicant's loan details below to check if they're at *High Risk* or *Low Risk*.")

# Input form
with st.form("loan_form"):
    income = st.number_input("Annual Income (USD)", min_value=0)
    age = st.slider("Age", 18, 75, 30)
    experience = st.slider("Years of Work Experience", 0, 50, 5)
    married = st.selectbox("Marital Status", ["Yes", "No"])
    house_ownership = st.selectbox("House Ownership", ["Rented", "Owned", "Neither"])
    car_owned = st.selectbox("Owns a Car?", ["Yes", "No"])
    current_job_years = st.slider("Years in Current Job", 0, 20, 2)
    current_house_years = st.slider("Years in Current House", 0, 30, 5)
    city_code = st.number_input("City Code (integer)", min_value=0)
    state_code = st.number_input("State Code (integer)", min_value=0)

    submitted = st.form_submit_button("Predict")

if submitted:
    # Map categorical to numerical (ensure this matches your training data!)
    married = 1 if married == "Yes" else 0
    car_owned = 1 if car_owned == "Yes" else 0
    house_dict = {"Rented": 0, "Owned": 1, "Neither": 2}
    house_ownership = house_dict[house_ownership]

    # Create dataframe
    input_data = pd.DataFrame([[
        income, age, experience, married, house_ownership,
        car_owned, current_job_years, current_house_years, city_code, state_code
    ]], columns=[
        "income", "age", "experience", "married", "house_ownership", "car_owned",
        "current_job_years", "current_house_years", "city", "state"
    ])

    # Predict
    prediction = model.predict(input_data)[0]
    prob = model.predict_proba(input_data)[0][1]  # Probability of being high risk

    # Result
    st.subheader("üîç Prediction Result")
    if prediction == 1:
        st.error(f"‚ö†Ô∏è High Risk of Loan Default with probability {prob:.2f}")
    else:
        st.success(f"‚úÖ Low Risk of Loan Default with probability {1 - prob:.2f}")

    # Feature importance plot
    st.subheader("üìä Feature Importance")
    fig, ax = plt.subplots(figsize=(10, 6))
    plot_importance(model.named_steps['model'], max_num_features=10, height=0.5, ax=ax)
    st.pyplot(fig)

!pip install streamlit